{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMm420sjvXvbBHnXk3OrwtY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiri9/Federated-Machine-Learning/blob/main/FL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLCf-P7n4nlG",
        "outputId": "ae45b12c-0ad1-4027-8f21-0c6f0917905a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.4/200.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.1/98.1 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for gpustat (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q flwr[simulation] torch torchvision matplotlib;"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "#from torchvision.datasets import MNIST\n",
        "from torch.utils.data import ConcatDataset, DataLoader, Dataset, Subset, random_split\n",
        "\n",
        "import flwr as fl\n",
        "from flwr.common import Metrics\n",
        "from flwr.common.typing import NDArrays, Scalar\n",
        "\n",
        "DEVICE = torch.device(\"cpu\")  # Try \"cuda\" to train on GPU\n",
        "print(\n",
        "    f\"Training on {DEVICE} using PyTorch {torch.__version__} and Flower {fl.__version__}\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "694Z6FJs4ynR",
        "outputId": "8b2b5cbd-2cfc-41b0-b6e3-178475d569da"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on cpu using PyTorch 2.0.1+cu118 and Flower 1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "NUM_CLIENTS = 10\n",
        "BATCH_SIZE = 32\n",
        "VALID_FRACTION = 0.2 # fraction of the dataset used for each local client\n",
        "NUM_ROUNDS = 5"
      ],
      "metadata": {
        "id": "IJNAAWvGa1sM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VayP31HX48uz",
        "outputId": "df6217d4-bdcb-43d7-9201-65c5cfd1cfe5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reading training csv file from google drive\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/kdd_train.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/kdd_test.csv')\n",
        "\n",
        "#creating copies of datasets\n",
        "df_train_copy = df_train.copy()\n",
        "df_test_copy = df_test.copy()\n",
        "\n",
        "#Encoding categories in 0-Normal, 1-DoS, 2-Probe, 3-R2L, 4-U2R\n",
        "df1_train_labels = df_train_copy['labels']\n",
        "df1_train_labels_in_numbers = df1_train_labels.replace({ 'normal' : 0, 'neptune' : 1,'land' : 1, 'back': 1, 'teardrop': 1, 'pod': 1, 'smurf' : 1,\n",
        "                                                     'ipsweep' : 2, 'nmap' : 2, 'portsweep' : 2, 'satan' : 2,\n",
        "                                                     'phf': 3, 'multihop': 3, 'warezclient': 3,'warezmaster': 3, 'spy': 3, 'ftp_write' : 3,\n",
        "                                                     'guess_passwd': 3,'imap': 3,\n",
        "                                                     'buffer_overflow': 4, 'loadmodule': 4,'perl': 4,  'rootkit': 4 })\n",
        "#replacing the string output clsses by numbers\n",
        "df_train_copy['labels'] = df1_train_labels_in_numbers\n",
        "\n",
        "#Encoding categories in 0-Normal, 1-DoS, 2-Probe, 3-R2L, 4-U2R\n",
        "df1_test_labels = df_test_copy['labels']\n",
        "df1_test_labels_in_numbers = df1_test_labels.replace({ 'normal' : 0, 'neptune' : 1 ,'back': 1, 'land': 1, 'pod': 1, 'smurf': 1,\n",
        "                                                      'teardrop': 1,'mailbomb': 1, 'apache2': 1, 'processtable': 1,\n",
        "                                                     'ipsweep' : 2, 'nmap' : 2, 'portsweep' : 2, 'satan' : 2, 'mscan' : 2,'saint' : 2,\n",
        "                                                     'ftp_write': 3,'guess_passwd': 3,'imap': 3,'multihop': 3,'phf': 3, 'warezclient': 3,\n",
        "                                                       'warezmaster': 3,'sendmail': 3,'named': 3,'snmpgetattack': 3,'snmpguess': 3,'xlock': 3,\n",
        "                                                       'xsnoop': 3,'httptunnel': 3,\n",
        "                                                     'buffer_overflow': 4, 'loadmodule': 4,'perl': 4,  'rootkit': 4, 'ps': 4,'xterm': 4 })\n",
        "#replacing the string output clsses by numbers\n",
        "df_test_copy['labels'] = df1_test_labels_in_numbers\n",
        "\n",
        "#Transform categorical features into numbers using LabelEncoder() from train dataset\n",
        "dft = df_train_copy.apply(LabelEncoder().fit_transform)\n",
        "df_train_copy = dft\n",
        "\n",
        "#Transform categorical features into numbers using LabelEncoder() from test dataset\n",
        "dftt = df_test_copy.apply(LabelEncoder().fit_transform)\n",
        "df_test_copy = dftt\n",
        "\n",
        "# Dropping the column \"num_outbound_cmds\" from train and test datasets\n",
        "df_train_copy = df_train_copy.drop('num_outbound_cmds', axis=1)\n",
        "df_test_copy = df_test_copy.drop('num_outbound_cmds', axis=1)\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Select the columns to be normalized (assuming they are numerical features)\n",
        "numerical_columns = ['duration', 'service', 'flag', 'src_bytes', 'dst_bytes', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'same_srv_rate',\n",
        "                     'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
        "                     'dst_host_same_src_port_rate','dst_host_serror_rate', 'dst_host_srv_serror_rate', 'rerror_rate',\n",
        "                     'srv_rerror_rate','diff_srv_rate', 'srv_diff_host_rate', 'dst_host_srv_diff_host_rate',\n",
        "                     'dst_host_rerror_rate', 'dst_host_srv_rerror_rate','hot','num_compromised','num_root',]\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the scaler to the selected columns\n",
        "scaler.fit(df_train_copy[numerical_columns])\n",
        "\n",
        "# Transform the selected columns with the scaler\n",
        "df_train_copy[numerical_columns] = scaler.transform(df_train_copy[numerical_columns])\n",
        "\n",
        "# Select the columns to be normalized (assuming they are numerical features)\n",
        "numerical_columns = ['duration', 'service', 'flag', 'src_bytes', 'dst_bytes', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'same_srv_rate',\n",
        "                     'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
        "                     'dst_host_same_src_port_rate','dst_host_serror_rate', 'dst_host_srv_serror_rate', 'rerror_rate',\n",
        "                     'srv_rerror_rate','diff_srv_rate', 'srv_diff_host_rate', 'dst_host_srv_diff_host_rate',\n",
        "                     'dst_host_rerror_rate', 'dst_host_srv_rerror_rate','hot','num_compromised','num_root',]\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the scaler to the selected columns\n",
        "scaler.fit(df_test_copy[numerical_columns])\n",
        "\n",
        "# Transform the selected columns with the scaler\n",
        "df_test_copy[numerical_columns] = scaler.transform(df_test_copy[numerical_columns])"
      ],
      "metadata": {
        "id": "F6V_PbdJ5Fc6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_copy.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-b7xYQPwOpj3",
        "outputId": "d869f6d9-6963-448f-a807-1617a336d75b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22544, 41)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "NUM_CLIENTS = 10\n",
        "\n",
        "\n",
        "def load_datasets():\n",
        "\n",
        "    # Assuming \"label\" is the name of your label column\n",
        "    df_train_labels = df_train_copy[\"labels\"]\n",
        "    df_test_labels = df_test_copy[\"labels\"]\n",
        "\n",
        "    # Convert your DataFrames into PyTorch datasets\n",
        "    train_dataset = TensorDataset(torch.Tensor(df_train_copy.values), torch.Tensor(df_train_labels.values))\n",
        "    test_dataset = TensorDataset(torch.Tensor(df_test_copy.values), torch.Tensor(df_test_labels.values))\n",
        "\n",
        "    # Split training set into 10 partitions to simulate the individual dataset\n",
        "    partition_size = len(train_dataset) // NUM_CLIENTS\n",
        "    # Initialize a list to store the partitions\n",
        "    data_partitions = []\n",
        "\n",
        "    # Split the data into partitions\n",
        "    for i in range(NUM_CLIENTS):\n",
        "        start_index = i * partition_size\n",
        "        end_index = (i + 1) * partition_size if i < NUM_CLIENTS - 1 else len(df_train_copy)\n",
        "        partition = df_train_copy.iloc[start_index:end_index]\n",
        "        data_partitions.append(partition)\n",
        "\n",
        "    # Now, data_partitions contains 10 partitions of roughly equal size.\n",
        "\n",
        "    # Split each partition into train/val and create DataLoader\n",
        "    trainloaders = []\n",
        "    valloaders = []\n",
        "    for ds in data_partitions:\n",
        "        len_val = len(ds) // 10  # 10 % validation set\n",
        "        len_train = len(ds) - len_val\n",
        "        lengths = [len_train, len_val]\n",
        "        ds_train, ds_val = random_split(ds, lengths, generator=torch.Generator().manual_seed(42))\n",
        "        trainloaders.append(DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True))\n",
        "        valloaders.append(DataLoader(ds_val, batch_size=BATCH_SIZE))\n",
        "    testloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "    return trainloaders, valloaders, testloader\n",
        "\n",
        "trainloaders, valloaders, testloader = load_datasets()"
      ],
      "metadata": {
        "id": "73q97WdW5apm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    \"\"\"Basic CNN implementation\"\"\"\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 5, padding=\"same\")\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 5, padding=\"same\")\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 2048)\n",
        "        self.fc2 = nn.Linear(2048, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 7 * 7 * 64)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "uXq7eTlCJGrw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, trainloader, epochs: int, verbose=False):\n",
        "    \"\"\"Train the network on the training set.\"\"\"\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters())\n",
        "    net.train()\n",
        "    for epoch in range(epochs):\n",
        "        correct, total, epoch_loss = 0, 0, 0.0\n",
        "        for inputs, labels in trainloader:\n",
        "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # Metrics\n",
        "            epoch_loss += loss\n",
        "            total += labels.size(0)\n",
        "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
        "        epoch_loss /= len(trainloader.dataset)\n",
        "        epoch_acc = correct / total\n",
        "        if verbose:\n",
        "            print(f\"Epoch {epoch+1}: train loss {epoch_loss}, accuracy {epoch_acc}\")\n",
        "\n",
        "\n",
        "def test(net, testloader):\n",
        "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    correct, total, loss = 0, 0, 0.0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = net(inputs)\n",
        "            loss += criterion(outputs, labels).item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    loss /= len(testloader.dataset)\n",
        "    accuracy = correct / total\n",
        "    return loss, accuracy"
      ],
      "metadata": {
        "id": "cHxG6auRJSsM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from flwr.common.typing import NDArray, Scalar\n",
        "from typing import Dict, Tuple\n",
        "import numpy as np\n",
        "\n",
        "class FlowerClient(fl.client.NumPyClient):\n",
        "    def __init__(self, net: nn.Module, trainloader: DataLoader, valloader: DataLoader):\n",
        "        self.net = net\n",
        "        self.trainloader = trainloader\n",
        "        self.valloader = valloader\n",
        "\n",
        "    def get_parameters(self) -> NDArray:\n",
        "        \"\"\"Return the current local model parameters as a flat 1D array\"\"\"\n",
        "        return np.concatenate([p.cpu().numpy().flatten() for p in self.net.parameters()])\n",
        "\n",
        "    def fit(self, parameters: NDArray, config: Dict[str, Scalar]) -> NDArray:\n",
        "        \"\"\"Train the model on the local (train) data.\"\"\"\n",
        "        # Set the received parameters to the model\n",
        "        self.set_parameters(parameters)\n",
        "\n",
        "        # Define the loss and optimizer\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(self.net.parameters())\n",
        "\n",
        "        # Training loop\n",
        "        self.net.train()\n",
        "        for epoch in range(config['epochs']):\n",
        "            for inputs, labels in self.trainloader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.net(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        # Return the updated model parameters\n",
        "        return self.get_parameters()\n",
        "\n",
        "    def evaluate(self, parameters: NDArray, config: Dict[str, Scalar]) -> Tuple[float, int, Dict[str, Scalar]]:\n",
        "        \"\"\"Evaluate the model using the validation data.\"\"\"\n",
        "        # Set the received parameters to the model\n",
        "        self.set_parameters(parameters)\n",
        "\n",
        "        # Define the loss and initialize evaluation metrics\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "        total_loss = 0.0\n",
        "        correct = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        # Evaluation loop\n",
        "        self.net.eval()\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in self.valloader:\n",
        "                outputs = self.net(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                total_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total_samples += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Calculate accuracy and return evaluation results\n",
        "        accuracy = correct / total_samples\n",
        "        avg_loss = total_loss / len(self.valloader)\n",
        "\n",
        "        metrics = {\"accuracy\": accuracy}\n",
        "        return avg_loss, total_samples, metrics\n",
        "\n",
        "    def set_parameters(self, parameters: NDArray) -> None:\n",
        "        \"\"\"Set the received parameters to the model.\"\"\"\n",
        "        offset = 0\n",
        "        for p in self.net.parameters():\n",
        "            length = np.prod(p.shape)\n",
        "            p.data = torch.from_numpy(parameters[offset:offset+length].reshape(p.shape))\n",
        "            offset += length"
      ],
      "metadata": {
        "id": "nFOwBnwtW8bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a PyTorch neural network\n",
        "net = Net().to(DEVICE)\n",
        "\n",
        "# Train and evaluate the network on each client\n",
        "for epoch in range(5):\n",
        "    for i in range(NUM_CLIENTS):\n",
        "        trainloader = trainloaders[i]\n",
        "        valloader = valloaders[i]\n",
        "        train(net, trainloader, epoch)\n",
        "        loss, accuracy = test(net, valloader)\n",
        "        print(f\"Client {i+1}: validation loss {loss}, accuracy {accuracy}\")\n",
        "\n",
        "# Evaluate the network on the test set\n",
        "loss, accuracy = test(net, test_dataset)\n",
        "print(f\"Final test set performance:\\n\\tloss {loss}\\n\\taccuracy {accuracy}\")"
      ],
      "metadata": {
        "id": "89e94GG3J832"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}