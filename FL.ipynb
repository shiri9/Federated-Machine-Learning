{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4JqhBvZ023Q7Dx6MYQsdD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiri9/Federated-Machine-Learning/blob/main/FL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VLCf-P7n4nlG"
      },
      "outputs": [],
      "source": [
        "!pip install -q flwr[simulation] torch torchvision matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "from typing import List, Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
        "\n",
        "import flwr as fl\n",
        "from flwr.common import Metrics\n",
        "\n",
        "DEVICE = torch.device(\"cpu\")  # Try \"cuda\" to train on GPU\n",
        "print(\n",
        "    f\"Training on {DEVICE} using PyTorch {torch.__version__} and Flower {fl.__version__}\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "694Z6FJs4ynR",
        "outputId": "3f88743f-da29-4f02-bbd0-ecf961f29ac3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on cpu using PyTorch 2.0.1+cu118 and Flower 1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VayP31HX48uz",
        "outputId": "df6217d4-bdcb-43d7-9201-65c5cfd1cfe5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reading training csv file from google drive\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/kdd_train.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/kdd_test.csv')\n",
        "\n",
        "#creating copies of datasets\n",
        "df_train_copy = df_train.copy()\n",
        "df_test_copy = df_test.copy()\n",
        "\n",
        "#Encoding categories in 0-Normal, 1-DoS, 2-Probe, 3-R2L, 4-U2R\n",
        "df1_train_labels = df_train_copy['labels']\n",
        "df1_train_labels_in_numbers = df1_train_labels.replace({ 'normal' : 0, 'neptune' : 1,'land' : 1, 'back': 1, 'teardrop': 1, 'pod': 1, 'smurf' : 1,\n",
        "                                                     'ipsweep' : 2, 'nmap' : 2, 'portsweep' : 2, 'satan' : 2,\n",
        "                                                     'phf': 3, 'multihop': 3, 'warezclient': 3,'warezmaster': 3, 'spy': 3, 'ftp_write' : 3,\n",
        "                                                     'guess_passwd': 3,'imap': 3,\n",
        "                                                     'buffer_overflow': 4, 'loadmodule': 4,'perl': 4,  'rootkit': 4 })\n",
        "#replacing the string output clsses by numbers\n",
        "df_train_copy['labels'] = df1_train_labels_in_numbers\n",
        "\n",
        "#Encoding categories in 0-Normal, 1-DoS, 2-Probe, 3-R2L, 4-U2R\n",
        "df1_test_labels = df_test_copy['labels']\n",
        "df1_test_labels_in_numbers = df1_test_labels.replace({ 'normal' : 0, 'neptune' : 1 ,'back': 1, 'land': 1, 'pod': 1, 'smurf': 1,\n",
        "                                                      'teardrop': 1,'mailbomb': 1, 'apache2': 1, 'processtable': 1,\n",
        "                                                     'ipsweep' : 2, 'nmap' : 2, 'portsweep' : 2, 'satan' : 2, 'mscan' : 2,'saint' : 2,\n",
        "                                                     'ftp_write': 3,'guess_passwd': 3,'imap': 3,'multihop': 3,'phf': 3, 'warezclient': 3,\n",
        "                                                       'warezmaster': 3,'sendmail': 3,'named': 3,'snmpgetattack': 3,'snmpguess': 3,'xlock': 3,\n",
        "                                                       'xsnoop': 3,'httptunnel': 3,\n",
        "                                                     'buffer_overflow': 4, 'loadmodule': 4,'perl': 4,  'rootkit': 4, 'ps': 4,'xterm': 4 })\n",
        "#replacing the string output clsses by numbers\n",
        "df_test_copy['labels'] = df1_test_labels_in_numbers\n",
        "\n",
        "#Transform categorical features into numbers using LabelEncoder() from train dataset\n",
        "dft = df_train_copy.apply(LabelEncoder().fit_transform)\n",
        "df_train_copy = dft\n",
        "\n",
        "#Transform categorical features into numbers using LabelEncoder() from test dataset\n",
        "dftt = df_test_copy.apply(LabelEncoder().fit_transform)\n",
        "df_test_copy = dftt\n",
        "\n",
        "# Dropping the column \"num_outbound_cmds\" from train and test datasets\n",
        "df_train_copy = df_train_copy.drop('num_outbound_cmds', axis=1)\n",
        "df_test_copy = df_test_copy.drop('num_outbound_cmds', axis=1)\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Select the columns to be normalized (assuming they are numerical features)\n",
        "numerical_columns = ['duration', 'service', 'flag', 'src_bytes', 'dst_bytes', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'same_srv_rate',\n",
        "                     'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
        "                     'dst_host_same_src_port_rate','dst_host_serror_rate', 'dst_host_srv_serror_rate', 'rerror_rate',\n",
        "                     'srv_rerror_rate','diff_srv_rate', 'srv_diff_host_rate', 'dst_host_srv_diff_host_rate',\n",
        "                     'dst_host_rerror_rate', 'dst_host_srv_rerror_rate','hot','num_compromised','num_root',]\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the scaler to the selected columns\n",
        "scaler.fit(df_train_copy[numerical_columns])\n",
        "\n",
        "# Transform the selected columns with the scaler\n",
        "df_train_copy[numerical_columns] = scaler.transform(df_train_copy[numerical_columns])\n",
        "\n",
        "# Select the columns to be normalized (assuming they are numerical features)\n",
        "numerical_columns = ['duration', 'service', 'flag', 'src_bytes', 'dst_bytes', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'same_srv_rate',\n",
        "                     'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
        "                     'dst_host_same_src_port_rate','dst_host_serror_rate', 'dst_host_srv_serror_rate', 'rerror_rate',\n",
        "                     'srv_rerror_rate','diff_srv_rate', 'srv_diff_host_rate', 'dst_host_srv_diff_host_rate',\n",
        "                     'dst_host_rerror_rate', 'dst_host_srv_rerror_rate','hot','num_compromised','num_root',]\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the scaler to the selected columns\n",
        "scaler.fit(df_test_copy[numerical_columns])\n",
        "\n",
        "# Transform the selected columns with the scaler\n",
        "df_test_copy[numerical_columns] = scaler.transform(df_test_copy[numerical_columns])"
      ],
      "metadata": {
        "id": "F6V_PbdJ5Fc6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_copy.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-b7xYQPwOpj3",
        "outputId": "d869f6d9-6963-448f-a807-1617a336d75b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22544, 41)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "NUM_CLIENTS = 10\n",
        "\n",
        "\n",
        "def load_datasets():\n",
        "\n",
        "    # Assuming \"label\" is the name of your label column\n",
        "    df_train_labels = df_train_copy[\"labels\"]\n",
        "    df_test_labels = df_test_copy[\"labels\"]\n",
        "\n",
        "    # Convert your DataFrames into PyTorch datasets\n",
        "    train_dataset = TensorDataset(torch.Tensor(df_train_copy.values), torch.Tensor(df_train_labels.values))\n",
        "    test_dataset = TensorDataset(torch.Tensor(df_test_copy.values), torch.Tensor(df_test_labels.values))\n",
        "\n",
        "    # Split training set into 10 partitions to simulate the individual dataset\n",
        "    partition_size = len(train_dataset) // NUM_CLIENTS\n",
        "    # Initialize a list to store the partitions\n",
        "    data_partitions = []\n",
        "\n",
        "    # Split the data into partitions\n",
        "    for i in range(NUM_CLIENTS):\n",
        "        start_index = i * partition_size\n",
        "        end_index = (i + 1) * partition_size if i < NUM_CLIENTS - 1 else len(df_train_copy)\n",
        "        partition = df_train_copy.iloc[start_index:end_index]\n",
        "        data_partitions.append(partition)\n",
        "\n",
        "    # Now, data_partitions contains 10 partitions of roughly equal size.\n",
        "\n",
        "    # Split each partition into train/val and create DataLoader\n",
        "    trainloaders = []\n",
        "    valloaders = []\n",
        "    for ds in data_partitions:\n",
        "        len_val = len(ds) // 10  # 10 % validation set\n",
        "        len_train = len(ds) - len_val\n",
        "        lengths = [len_train, len_val]\n",
        "        ds_train, ds_val = random_split(ds, lengths, generator=torch.Generator().manual_seed(42))\n",
        "        trainloaders.append(DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True))\n",
        "        valloaders.append(DataLoader(ds_val, batch_size=BATCH_SIZE))\n",
        "    testloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "    return trainloaders, valloaders, testloader\n",
        "\n",
        "trainloaders, valloaders, testloader = load_datasets()"
      ],
      "metadata": {
        "id": "73q97WdW5apm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "uXq7eTlCJGrw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, trainloader, epochs: int, verbose=False):\n",
        "    \"\"\"Train the network on the training set.\"\"\"\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters())\n",
        "    net.train()\n",
        "    for epoch in range(epochs):\n",
        "        correct, total, epoch_loss = 0, 0, 0.0\n",
        "        for inputs, labels in trainloader:\n",
        "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # Metrics\n",
        "            epoch_loss += loss\n",
        "            total += labels.size(0)\n",
        "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
        "        epoch_loss /= len(trainloader.dataset)\n",
        "        epoch_acc = correct / total\n",
        "        if verbose:\n",
        "            print(f\"Epoch {epoch+1}: train loss {epoch_loss}, accuracy {epoch_acc}\")\n",
        "\n",
        "\n",
        "def test(net, testloader):\n",
        "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    correct, total, loss = 0, 0, 0.0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = net(inputs)\n",
        "            loss += criterion(outputs, labels).item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    loss /= len(testloader.dataset)\n",
        "    accuracy = correct / total\n",
        "    return loss, accuracy"
      ],
      "metadata": {
        "id": "cHxG6auRJSsM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a PyTorch neural network\n",
        "net = Net().to(DEVICE)\n",
        "\n",
        "# Train and evaluate the network on each client\n",
        "for epoch in range(5):\n",
        "    for i in range(NUM_CLIENTS):\n",
        "        trainloader = trainloaders[i]\n",
        "        valloader = valloaders[i]\n",
        "        train(net, trainloader, epoch)\n",
        "        loss, accuracy = test(net, valloader)\n",
        "        print(f\"Client {i+1}: validation loss {loss}, accuracy {accuracy}\")\n",
        "\n",
        "# Evaluate the network on the test set\n",
        "loss, accuracy = test(net, test_dataset)\n",
        "print(f\"Final test set performance:\\n\\tloss {loss}\\n\\taccuracy {accuracy}\")"
      ],
      "metadata": {
        "id": "89e94GG3J832"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}